{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training: Regularization\n",
    "\n",
    "Regularization in deep learning is a crucial technique to prevent overfitting, where a model performs well on training data but poorly on unseen data. Overfitting often occurs in complex models like deep neural networks that have a large number of parameters. Regularization methods add certain constraints to the model or modify the learning process to enhance its generalization capabilities.\n",
    "\n",
    "#### Motivation for Regularizers\n",
    "\n",
    "1. **Model Complexity:** Deep learning models, due to their high flexibility and capacity, can easily capture noise and random fluctuations in the training data. This leads to overfitting.\n",
    "2. **Generalization:** The goal of a model is not just to learn the training data but to make accurate predictions on new, unseen data. Regularization techniques help in achieving this by making the model less sensitive to individual fluctuations in the training data.\n",
    "3. **Improving Robustness:** Regularization can also improve the robustness of models, making them less likely to be influenced by small variations or corrupted inputs.\n",
    "\n",
    "Here, we'll cover L1 and L2 Regularization, Dropout, and max norm regularization.\n",
    "\n",
    "\n",
    "### L1 and L2 Regularization\n",
    "\n",
    "L1 and L2 regularization are used to penalize large weights in linear model, and are especially important when dealing with high-dimensional data or scenarios where overfitting is a concern. Let's explore these concepts starting with their application in linear models.\n",
    "\n",
    "#### L1 Regularization (Lasso)\n",
    "\n",
    "1. **Concept:**\n",
    "   - L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients to the loss function.\n",
    "   - Mathematically, for a linear model with weights $ \\mathbf{w} $, the L1 regularized loss is: $ \\text{Loss} = \\text{MSE} + \\lambda \\sum |w_i| $, where MSE is Mean Squared Error and $ \\lambda $ is the regularization parameter.\n",
    "\n",
    "2. **Sparsity:**\n",
    "   - L1 regularization tends to produce sparse models, where some coefficients can become exactly zero. This is useful for feature selection, making L1 regularization a tool for identifying important features in the data.\n",
    "\n",
    "3. **Use Cases:**\n",
    "   - Particularly beneficial in scenarios where some features are irrelevant or redundant.\n",
    "\n",
    "#### L2 Regularization (Ridge)\n",
    "\n",
    "1. **Concept:**\n",
    "   - L2 regularization adds a penalty equal to the square of the magnitude of coefficients.\n",
    "   - The L2 regularized loss for a linear model is: $ \\text{Loss} = \\text{MSE} + \\lambda \\sum w_i^2 $.\n",
    "\n",
    "2. **Weight Shrinking:**\n",
    "   - Unlike L1, L2 regularization doesnâ€™t lead to sparse models. Instead, it shrinks the weights towards zero but rarely reaches zero. This leads to smaller and more distributed weight values, which helps in preventing overfitting.\n",
    "\n",
    "3. **Use Cases:**\n",
    "   - Ideal for problems where all features have relevance or when you have collinear (correlated) features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t1a, t1b, t2a, t2b = -1, 3, -1.5, 1.5\n",
    "\n",
    "t1s = np.linspace(t1a, t1b, 500)\n",
    "t2s = np.linspace(t2a, t2b, 500)\n",
    "t1, t2 = np.meshgrid(t1s, t2s)\n",
    "T = np.c_[t1.ravel(), t2.ravel()]\n",
    "Xr = np.array([[1, 1], [1, -1], [1, 0.5]])\n",
    "yr = 2 * Xr[:, :1] + 0.5 * Xr[:, 1:]\n",
    "\n",
    "J = (1 / len(Xr) * ((T @ Xr.T - yr.T) ** 2).sum(axis=1)).reshape(t1.shape)\n",
    "\n",
    "N1 = np.linalg.norm(T, ord=1, axis=1).reshape(t1.shape)\n",
    "N2 = np.linalg.norm(T, ord=2, axis=1).reshape(t1.shape)\n",
    "\n",
    "t_min_idx = np.unravel_index(J.argmin(), J.shape)\n",
    "t1_min, t2_min = t1[t_min_idx], t2[t_min_idx]\n",
    "\n",
    "t_init = np.array([[0.25], [-1]])\n",
    "\n",
    "def bgd_path(theta, X, y, l1, l2, core=1, eta=0.05, n_iterations=200):\n",
    "    path = [theta]\n",
    "    for iteration in range(n_iterations):\n",
    "        gradients = (core * 2 / len(X) * X.T @ (X @ theta - y)\n",
    "                     + l1 * np.sign(theta) + l2 * theta)\n",
    "        theta = theta - eta * gradients\n",
    "        path.append(theta)\n",
    "    return np.array(path)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(10.1, 8))\n",
    "\n",
    "for i, N, l1, l2, title in ((0, N1, 2.0, 0, \"Lasso\"), (1, N2, 0, 2.0, \"Ridge\")):\n",
    "    JR = J + l1 * N1 + l2 * 0.5 * N2 ** 2\n",
    "\n",
    "    tr_min_idx = np.unravel_index(JR.argmin(), JR.shape)\n",
    "    t1r_min, t2r_min = t1[tr_min_idx], t2[tr_min_idx]\n",
    "\n",
    "    levels = np.exp(np.linspace(0, 1, 20)) - 1\n",
    "    levelsJ = levels * (J.max() - J.min()) + J.min()\n",
    "    levelsJR = levels * (JR.max() - JR.min()) + JR.min()\n",
    "    levelsN = np.linspace(0, N.max(), 10)\n",
    "\n",
    "    path_J = bgd_path(t_init, Xr, yr, l1=0, l2=0)\n",
    "    path_JR = bgd_path(t_init, Xr, yr, l1, l2)\n",
    "    path_N = bgd_path(theta=np.array([[2.0], [0.5]]), X=Xr, y=yr,\n",
    "                      l1=np.sign(l1) / 3, l2=np.sign(l2), core=0)\n",
    "    ax = axes[i, 0]\n",
    "    ax.grid()\n",
    "    ax.axhline(y=0, color=\"k\")\n",
    "    ax.axvline(x=0, color=\"k\")\n",
    "    ax.contourf(t1, t2, N / 2.0, levels=levelsN)\n",
    "    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n",
    "    ax.plot(0, 0, \"ys\")\n",
    "    ax.plot(t1_min, t2_min, \"ys\")\n",
    "    ax.set_title(fr\"$\\ell_{i + 1}$ penalty\")\n",
    "    ax.axis([t1a, t1b, t2a, t2b])\n",
    "    if i == 1:\n",
    "        ax.set_xlabel(r\"$\\theta_1$\")\n",
    "    ax.set_ylabel(r\"$\\theta_2$\", rotation=0)\n",
    "\n",
    "    ax = axes[i, 1]\n",
    "    ax.grid()\n",
    "    ax.axhline(y=0, color=\"k\")\n",
    "    ax.axvline(x=0, color=\"k\")\n",
    "    ax.contourf(t1, t2, JR, levels=levelsJR, alpha=0.9)\n",
    "    ax.plot(path_JR[:, 0], path_JR[:, 1], \"w-o\")\n",
    "    ax.plot(path_N[:, 0], path_N[:, 1], \"y--\")\n",
    "    ax.plot(0, 0, \"ys\")\n",
    "    ax.plot(t1_min, t2_min, \"ys\")\n",
    "    ax.plot(t1r_min, t2r_min, \"rs\")\n",
    "    ax.set_title(title)\n",
    "    ax.axis([t1a, t1b, t2a, t2b])\n",
    "    if i == 1:\n",
    "        ax.set_xlabel(r\"$\\theta_1$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Extending L1/L2 Regularization to Deep Learning\n",
    "\n",
    "In deep learning, L1 and L2 regularization concepts extend beyond linear models to complex networks. Just like in linear models, L1 and L2 regularization in neural networks involve adding a penalty term to the loss function. This penalty is calculated based on the weights of the network layers.\n",
    "\n",
    "- **Choice Between L1 and L2:**\n",
    "  - The choice between L1 and L2 regularization in neural networks can depend on the network architecture and the nature of the data. L2 is more common in deep learning due to its tendency to encourage smaller weights, thus smoothing the learned function.  However, L1 can lead to sparser networks, which require less memory and can run faster than in less sparse networks.\n",
    "  \n",
    "- **Hyperparameter Tuning:**\n",
    "  - The regularization parameter $ \\lambda $ controls the trade-off between fitting the training data and keeping the model weights small, and it's usually tuned using cross-validation.\n",
    "\n",
    "- **Impact on Learning:**\n",
    "  - Regularization can affect the learning dynamics. It's important to monitor both training and validation performance to understand the impact and adjust the regularization strength accordingly.  Importantly, L2 regularization is fine when using SGD, momentum optimization and Nesterov momentum optimization, but not with Adam and its variants. If you want to use Adam with weight decay, then do not use l2 regularization: use AdamW instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up some data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                              kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use `l1(0.1)` for l1 regularization with a factor of 0.1, or `l1_l2(0.1, 0.01)` for both l1 and l2 regularization, with factors 0.1 and 0.01 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42) \n",
    "\n",
    "# The following is fancy code that allows us to partially initialize an object \n",
    "\n",
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(tf.keras.layers.Dense,\n",
    "                           activation=\"relu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=2,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try different combinations of l1 and l2 regularizers (l1, l2, both, different alphas).  WHich seems to work the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Dropout\n",
    "\n",
    "Dropout is a remarkably effective and widely used regularization technique in deep neural networks, introduced by Geoffrey Hinton and further detailed by Nitish Srivastava et al. Its simplicity and effectiveness in enhancing network generalization have made it a staple in many state-of-the-art architectures.\n",
    "\n",
    "1. **Basic Idea:**\n",
    "   - During training, dropout randomly \"drops out\" (i.e., temporarily deactivates) neurons in the network with a certain probability $ p $, known as the dropout rate.\n",
    "   - This process is applied at every training step, and different neurons can be dropped in each step.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "\n",
    "![dropout](assets/dropout.png)\n",
    "</div>\n",
    "\n",
    "2. **Typical Dropout Rates:**\n",
    "   - The dropout rate $ p $ typically ranges between 10% and 50%, varying with the network type:\n",
    "     - Recurrent Neural Networks (RNNs): 20â€“30%\n",
    "     - Convolutional Neural Networks (CNNs): 40â€“50%\n",
    "\n",
    "#### Why Does Dropout Work?\n",
    "\n",
    "- **Redundancy Reduction:** Just like a company diversifying skillsets among employees, dropout forces the network to spread \"knowledge\" across its neurons, making it more robust and less reliant on specific neurons.\n",
    "- **Prevents Co-Adaptation:** Neurons become less sensitive to specific features or patterns in the input data, leading to a model that generalizes better.\n",
    "- **Ensemble Interpretation:** Conceptually, each training step with dropout is training a different \"thinned\" network. The final model can be seen as an averaging ensemble of these various networks.\n",
    "\n",
    "\n",
    "#### Adjusting Weights\n",
    "\n",
    "Due to the deactivation of neurons during training, the effective number of neurons participating in the network is reduced. To compensate for this reduction, it's necessary to scale up the active neurons. More specifically, each neuronâ€™s input connection weights are scaled by $ \\frac{1}{1 - p} $ during training. This scaling ensures that the total input to a neuron at test time (when dropout is not applied) matches the expected total input during training.\n",
    "\n",
    "#### Application in Deep Learning\n",
    "\n",
    "- **Use in Different Layers:**\n",
    "  - Dropout can be applied after dense layers, convolutional layers, or even recurrent layers, though the dropout rate might need adjustment depending on the layer type and the overall network architecture. In practice, you can usually apply dropout only to the neurons in the top one to three layers (excluding the output layer). \n",
    "\n",
    "- **Special Circumstances:**\n",
    "  - If you want to regularize a self-normalizing network based on the SELU activation function (as discussed earlier), you should use alpha dropout: this is a variant of dropout that preserves the mean and standard deviation of its inputs. It was introduced in the same paper as SELU, as regular dropout would break self-normalization.\n",
    "\n",
    "- **Balancing Dropout Rate:**\n",
    "  - The choice of dropout rate is crucial; too high, and the network might underfit; too low, and the regularization effect might be insufficient. This rate is often determined through cross-validation.\n",
    "\n",
    "- **Evaluation**\n",
    "  - Since dropout is only active during training, comparing the training loss and the validation loss can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So make sure to evaluate the training loss without dropout (e.g., after training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Keras, we add dropout by adding dropout layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
    "                          kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy looks like it's lower than the validation accuracy, but that's just because dropout is only active during training. If we evaluate the model on the training set after training (i.e., with dropout turned off), we get the \"real\" training accuracy, which is very slightly higher than the validation accuracy and the test accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** make sure to use AlphaDropout instead of Dropout if you want to build a self-normalizing neural net using SELU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Dropout\n",
    "\n",
    "Monte Carlo (MC) Dropout is an extension of the standard dropout technique, allowing it to be used not just as a regularizer during training but also as a tool for estimating model uncertainty during inference. This method was introduced in a paper by Yarin Gal and Zoubin Ghahramani, providing a theoretical foundation linking dropout with Bayesian approximation.\n",
    "\n",
    "1. **Standard Dropout:**\n",
    "   - In traditional dropout, random neurons are deactivated during training to prevent overfitting. However, during inference (model evaluation or prediction), dropout is usually disabled, and the network operates with all neurons active.\n",
    "\n",
    "2. **Monte Carlo Dropout:**\n",
    "   - MC Dropout proposes to keep dropout active even during inference. By doing so, it effectively turns the neural network into a probabilistic model.\n",
    "   - Each forward pass with dropout enabled generates slightly different predictions, capturing the model's uncertainty. By running the network multiple times (each time with a different dropout configuration), we can obtain a distribution of predictions.\n",
    "\n",
    "#### Theoretical Motivation\n",
    "\n",
    "- **Bayesian Approximation:**\n",
    "  - The key theoretical insight of MC Dropout is its interpretation as a form of Bayesian approximation. In Bayesian statistics, model uncertainty is captured by the distribution over possible models given the data.\n",
    "  - MC Dropout can be seen as approximating this Bayesian averaging process. Each dropout mask (pattern of dropped neurons) can be viewed as sampling from the space of possible network configurations, akin to sampling from a posterior distribution of models.\n",
    "  \n",
    "- **Estimating Uncertainty:**\n",
    "  - This process allows the network to express its \"uncertainty\" in predictions. For inputs where the model is less certain, the variance in the predictions across different dropout configurations will be higher.\n",
    "\n",
    "#### Implementation and Usage\n",
    "\n",
    "- **Implementation in Keras:**\n",
    "  - To implement MC Dropout, one simply continues to use dropout layers in the network as usual. However, during inference, these dropout layers are kept active.\n",
    "  - In Keras, this can be done by using the `training=True` argument when calling the model for prediction.\n",
    "\n",
    "- **Practical Considerations:**\n",
    "  - **Number of Forward Passes:** A key consideration is the number of forward passes to average over. More passes provide a better approximation of the uncertainty but at the cost of increased computation.\n",
    "  - **Interpreting Results:** The spread of the predictions gives an insight into the modelâ€™s confidence. Wider spreads indicate lower confidence, and vice versa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All we have to do is set the \"training=True\" flag and use multiple models\n",
    "# Note that this is just \"bagging\" with a set of neural networks\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "y_probas = np.stack([model(X_test, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "model.predict(X_test[:1]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, the mean of all models is a bit less \"certain\"\n",
    "y_proba[0].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also examine std dev over all models to get an estimate of confidence\n",
    "y_std = y_probas.std(axis=0)\n",
    "y_std[0].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently thereâ€™s quite a lot of variance in the probability estimates for class 9: the standard deviation is 0.19, which should be compared to the estimated probability of 0.719: if you were building a risk-sensitive system (e.g., a medical or financial system), you would probably treat such an uncertain prediction with extreme caution. \n",
    "\n",
    "**Using MC Dropout in Practice**\n",
    "\n",
    "Note that in practice, other layers (e.g., BatchNormalization) might operate in special ways during training, so setting `training=true` is perhaps not what we want.  In order to handle this, it's relatively easy in Keras to create an MCDropout layer that turns on training during evaluation, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(tf.keras.layers.Dropout):\n",
    "    def call(self, inputs, training=None):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following converts all dropout layers to MCDropout\n",
    "# extra code â€“ shows how to convert Dropout to MCDropout in a Sequential model\n",
    "Dropout = tf.keras.layers.Dropout\n",
    "mc_model = tf.keras.Sequential([\n",
    "    MCDropout(layer.rate) if isinstance(layer, Dropout) else layer\n",
    "    for layer in model.layers\n",
    "])\n",
    "mc_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can just use our modified network without setting training to true\n",
    "tf.random.set_seed(42)\n",
    "np.mean([mc_model.predict(X_test[:1])\n",
    "         for sample in range(100)], axis=0).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Max-Norm Regularization\n",
    "\n",
    "Max-Norm regularization is a technique used in training neural networks that imposes a constraint on the weights of the neurons. This constraint limits the norm (i.e., the magnitude) of the vector of weights of incoming connections to a neuron.\n",
    "\n",
    "#### Concept and Mechanism\n",
    "\n",
    "1. **Max-Norm Constraint:**\n",
    "   - In Max-Norm regularization, for each neuron, the norm of its incoming weight vector is constrained to not exceed a predefined threshold, typically denoted as $ r $. \n",
    "   - Mathematically, if $ \\mathbf{w} $ is the weight vector for a neuron, then Max-Norm regularization ensures that $\\|\\mathbf{w}\\|_2 \\leq r $, where $\\|\\mathbf{w}\\|_2$ is the $ l_2 $-norm (Euclidean norm) of $ \\mathbf{w} $.\n",
    "\n",
    "2. **Application During Training:**\n",
    "   - After each training step (e.g., after a mini-batch update), the weight vectors of the neurons are checked. If the norm of a weight vector exceeds $ r $, it is scaled down to satisfy the Max-Norm constraint.\n",
    "\n",
    "#### Theoretical Motivation\n",
    "\n",
    "- **Preventing Exploding Weights:**\n",
    "  - Max-Norm regularization primarily helps in preventing the weights from growing too large, which can be a common issue leading to exploding gradients, especially in deep or recurrent neural networks.\n",
    "  - By keeping the weights small, it can help improve the overall stability of the learning process.\n",
    "\n",
    "- **Regularization Effect:**\n",
    "  - Similar to L1 and L2 regularization, Max-Norm has a regularizing effect, which helps in reducing overfitting and improving the generalization of the model.\n",
    "\n",
    "#### Implementation in Keras\n",
    "\n",
    "- **Using `kernel_constraint`:**\n",
    "  - In Keras, Max-Norm regularization is implemented using the `kernel_constraint` parameter in layer constructors. The `MaxNorm` constraint can be applied as follows:\n",
    "\n",
    "    ```python\n",
    "    from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(64, kernel_constraint=MaxNorm(max_value=2.0)))\n",
    "    ```\n",
    "\n",
    "#### Practical Considerations\n",
    "\n",
    "- **Choosing the Max-Norm Value (`r`):**\n",
    "  - The choice of the Max-Norm threshold $ r $ is a hyperparameter that can be tuned based on the specific problem and dataset. A smaller $ r $ imposes a stronger constraint on the weights.\n",
    "  - It's often determined through experimentation and cross-validation.\n",
    "\n",
    "- **Compatibility with Other Regularization Techniques:**\n",
    "  - Max-Norm regularization can be used alongside other regularization techniques, such as dropout or L1/L2 regularization, to compound the benefits.\n",
    "\n",
    "- **Effectiveness in Different Network Types:**\n",
    "  - While useful in a wide range of networks, Max-Norm regularization can be particularly effective in recurrent neural networks (RNNs), where exploding gradients are more common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how to implement max-norm in a single layer.\n",
    "\n",
    "dense = tf.keras.layers.Dense(\n",
    "    100, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "    kernel_constraint=tf.keras.constraints.max_norm(1.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, you can also use the \"partial\" method above to create a lightweight MaxNormDense layer\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "MaxNormDense = partial(tf.keras.layers.Dense,\n",
    "                       activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_constraint=tf.keras.constraints.max_norm(1.))\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(100),\n",
    "    MaxNormDense(100),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Build two networks for processing the MNIST Fashion data.  Both networks should have three dense layers (but may also have dropout layers or activation function layers) with 100 neurons.\n",
    "\n",
    "    1.  A self-normalizing neural network with dropout.\n",
    "    2.  A network without normalization but using dropout.\n",
    "    3.  A network with BatchNormalization.\n",
    "\n",
    "Try each with either a 1cycle or performance schedule.  Which works the best?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
